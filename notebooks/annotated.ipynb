{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os, sys\n",
    "import xml.etree.ElementTree as ET\n",
    "import xmltodict, json\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import PIL.ImageColor as ImageColor\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import PIL.ImageFont as ImageFont\n",
    "from time import sleep\n",
    "import psutil\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display image+bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_box_on_image(image, ymin, xmin, ymax, xmax,color='red',\n",
    "                               thickness=4, display_str_list=(), use_normalized_coordinates=True):\n",
    "  \"\"\"Adds a bounding box to an image.\n",
    "\n",
    "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
    "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
    "\n",
    "  Each string in display_str_list is displayed on a separate line above the\n",
    "  bounding box in black text on a rectangle filled with the input 'color'.\n",
    "  If the top of the bounding box extends to the edge of the image, the strings\n",
    "  are displayed below the bounding box.\n",
    "\n",
    "  Args:\n",
    "    image: a PIL.Image object.\n",
    "    ymin: ymin of bounding box.\n",
    "    xmin: xmin of bounding box.\n",
    "    ymax: ymax of bounding box.\n",
    "    xmax: xmax of bounding box.\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list: list of strings to display in box\n",
    "                      (each to be shown on its own line).\n",
    "    use_normalized_coordinates: If True (default), treat coordinates\n",
    "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
    "      coordinates as absolute.\n",
    "  \"\"\"\n",
    "  draw = ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  if use_normalized_coordinates:\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                  ymin * im_height, ymax * im_height)\n",
    "  else:\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "  draw.line([(left, top), (left, bottom), (right, bottom),\n",
    "             (right, top), (left, top)], width=thickness, fill=color)\n",
    "  try:\n",
    "    font = ImageFont.truetype('arial.ttf', 24)\n",
    "  except IOError:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "  # If the total height of the display strings added to the top of the bounding\n",
    "  # box exceeds the top of the image, stack the strings below the bounding box\n",
    "  # instead of above.\n",
    "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
    "  \n",
    "  # Each display_str has a top and bottom margin of 0.05x.\n",
    "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
    "\n",
    "  if top > total_display_str_height:\n",
    "    text_bottom = top\n",
    "  else:\n",
    "    text_bottom = bottom + total_display_str_height\n",
    "\n",
    "  # Reverse list and print from bottom to top.\n",
    "  for display_str in display_str_list[::-1]:\n",
    "    text_width, text_height = font.getsize(display_str)\n",
    "    margin = np.ceil(0.05 * text_height)\n",
    "    draw.rectangle([(left, text_bottom - text_height - 2 * margin), (left + text_width, text_bottom)], fill=color)\n",
    "    draw.text((left + margin, text_bottom - text_height - margin),\n",
    "              display_str, fill='black', font=font)\n",
    "    text_bottom -= text_height - 2 * margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List files in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_path = '../data/augmented'\n",
    "\n",
    "# list all files in directory\n",
    "files = sorted(os.listdir(annotated_path))\n",
    "n_files = len(files)\n",
    "print(\"Nr of files: \", n_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate annotated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 colours for the bounding boxes, based on the classes 1 to 5\n",
    "colours = {'one': 'red', 'two': 'green', 'three': 'blue', 'four': 'yellow', 'five': 'orange'}\n",
    "\n",
    "# storage path for final result\n",
    "save_path = '../data/annotated_combined/'\n",
    "\n",
    "# only use .xml files\n",
    "files_xml = [f for f in files if f.endswith('.xml')]\n",
    "\n",
    "for file_id, filename in tqdm(enumerate(files_xml), position=0, leave=True): \n",
    "    # XML object --> dict for the current file\n",
    "    # obj = xmltodict.parse(open(annotated_path + '/' + filename).read())\n",
    "\n",
    "    # read the original xml\n",
    "    xml_path = os.path.join(annotated_path, files_xml[file_id])\n",
    "    xml_file_orig = ET.parse(xml_path)\n",
    "    root = xml_file_orig.getroot()\n",
    "\n",
    "    # open image\n",
    "    image_pil = Image.open(annotated_path + '/' + filename.replace('xml', 'png'))\n",
    "\n",
    "    # # if there is only one object, wrap in list\n",
    "    # annotations = obj['annotation']['object']\n",
    "\n",
    "    # if not isinstance(annotations, list):\n",
    "    #     annotations = [annotations]\n",
    "\n",
    "    # print all objects in file\n",
    "    # loop over each bounding box\n",
    "    for obj in root.iter('object'):\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = bndbox.find('xmin')\n",
    "        xmax = bndbox.find('xmax')\n",
    "        ymin = bndbox.find('ymin')\n",
    "        ymax = bndbox.find('ymax')\n",
    "\n",
    "        xmin = int(xmin.text)\n",
    "        xmax = int(xmax.text)\n",
    "        ymin = int(ymin.text)\n",
    "        ymax = int(ymax.text)\n",
    "\n",
    "        # class name\n",
    "        class_name = obj.find('name').text\n",
    "\n",
    "        # print(\"At filename: \", filename)\n",
    "        # xmin = int(annotation['bndbox']['xmin'])\n",
    "        # ymin = int(annotation['bndbox']['ymin'])\n",
    "        # xmax = int(annotation['bndbox']['xmax'])\n",
    "        # ymax = int(annotation['bndbox']['ymax'])\n",
    "        # class_name = annotation['name']\n",
    "\n",
    "        # print(type(xmin), type(ymin), type(xmax), type(ymax))\n",
    "\n",
    "        # draw bounding boxes on image\n",
    "        draw_bounding_box_on_image(image_pil, ymin, xmin, ymax, xmax, colours[class_name],\n",
    "                                    1, display_str_list=[class_name], use_normalized_coordinates=False)\n",
    "\n",
    "        # save image\n",
    "        image_path = os.path.join(save_path, 'gt_' + str(file_id) + '_.png')\n",
    "        \n",
    "        image_pil.save(image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "summary(model, (3, 64, 64))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageList' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\lange\\.conda\\envs\\deepl\\lib\\site-packages\\torchsummaryX\\torchsummaryX.py:30\u001b[0m, in \u001b[0;36msummary.<locals>.register_hook.<locals>.hook\u001b[1;34m(module, inputs, outputs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchsummaryX/torchsummaryX.py?line=28'>29</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchsummaryX/torchsummaryX.py?line=29'>30</a>\u001b[0m     info[\u001b[39m\"\u001b[39m\u001b[39mout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(outputs[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49msize())\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchsummaryX/torchsummaryX.py?line=30'>31</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchsummaryX/torchsummaryX.py?line=31'>32</a>\u001b[0m     \u001b[39m# pack_padded_seq and pad_packed_seq store feature into data attribute\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ImageList' object has no attribute 'size'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lange\\OneDrive\\Projects\\2022\\finger-counting\\FingerCountingCV\\notebooks\\annotated.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lange/OneDrive/Projects/2022/finger-counting/FingerCountingCV/notebooks/annotated.ipynb#ch0000011?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mdetection\u001b[39m.\u001b[39mfasterrcnn_resnet50_fpn()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lange/OneDrive/Projects/2022/finger-counting/FingerCountingCV/notebooks/annotated.ipynb#ch0000011?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lange/OneDrive/Projects/2022/finger-counting/FingerCountingCV/notebooks/annotated.ipynb#ch0000011?line=7'>8</a>\u001b[0m summary(model, torch\u001b[39m.\u001b[39;49mzeros(\u001b[39m4\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m224\u001b[39;49m, \u001b[39m224\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\lange\\.conda\\envs\\deepl\\lib\\site-packages\\torchsummaryX\\torchsummaryX.py:86\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchsummaryX/torchsummaryX.py?line=83'>84</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchsummaryX/torchsummaryX.py?line=84'>85</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchsummaryX/torchsummaryX.py?line=85'>86</a>\u001b[0m         model(x) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (kwargs \u001b[39mor\u001b[39;00m args) \u001b[39melse\u001b[39;00m model(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchsummaryX/torchsummaryX.py?line=86'>87</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchsummaryX/torchsummaryX.py?line=87'>88</a>\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[1;32mc:\\Users\\lange\\.conda\\envs\\deepl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\lange\\.conda\\envs\\deepl\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:78\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchvision/models/detection/generalized_rcnn.py?line=74'>75</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(val) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchvision/models/detection/generalized_rcnn.py?line=75'>76</a>\u001b[0m     original_image_sizes\u001b[39m.\u001b[39mappend((val[\u001b[39m0\u001b[39m], val[\u001b[39m1\u001b[39m]))\n\u001b[1;32m---> <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchvision/models/detection/generalized_rcnn.py?line=77'>78</a>\u001b[0m images, targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(images, targets)\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchvision/models/detection/generalized_rcnn.py?line=79'>80</a>\u001b[0m \u001b[39m# Check for degenerate boxes\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchvision/models/detection/generalized_rcnn.py?line=80'>81</a>\u001b[0m \u001b[39m# TODO: Move this to a function\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchvision/models/detection/generalized_rcnn.py?line=81'>82</a>\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lange\\.conda\\envs\\deepl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m-> <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[0;32m   <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torch/nn/modules/module.py?line=1132'>1133</a>\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[1;32mc:\\Users\\lange\\.conda\\envs\\deepl\\lib\\site-packages\\torchsummaryX\\torchsummaryX.py:33\u001b[0m, in \u001b[0;36msummary.<locals>.register_hook.<locals>.hook\u001b[1;34m(module, inputs, outputs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchsummaryX/torchsummaryX.py?line=29'>30</a>\u001b[0m         info[\u001b[39m\"\u001b[39m\u001b[39mout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(outputs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize())\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchsummaryX/torchsummaryX.py?line=30'>31</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchsummaryX/torchsummaryX.py?line=31'>32</a>\u001b[0m         \u001b[39m# pack_padded_seq and pad_packed_seq store feature into data attribute\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchsummaryX/torchsummaryX.py?line=32'>33</a>\u001b[0m         info[\u001b[39m\"\u001b[39m\u001b[39mout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(outputs[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39msize())\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchsummaryX/torchsummaryX.py?line=33'>34</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/lange/.conda/envs/deepl/lib/site-packages/torchsummaryX/torchsummaryX.py?line=34'>35</a>\u001b[0m     info[\u001b[39m\"\u001b[39m\u001b[39mout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(outputs\u001b[39m.\u001b[39msize())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ImageList' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "from torchvision import models\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn()\n",
    "model.eval()\n",
    "summary(model, torch.zeros(4, 3, 224, 224))\n",
    "\n",
    "# # For inference\n",
    "# model.eval()\n",
    "\n",
    "# x = [torch.rand(3, 300, 400)]\n",
    "# print(x[0].shape)\n",
    "# predictions = model(x)           # Returns predictions\n",
    "# print(predictions)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369441202de9f090a0e57b38848d3e186c8686d751df00b39c544f960532016f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deepl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
