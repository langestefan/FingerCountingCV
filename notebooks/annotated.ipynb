{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os, sys\n",
    "import xml.etree.ElementTree as ET\n",
    "import xmltodict, json\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import PIL.ImageColor as ImageColor\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import PIL.ImageFont as ImageFont\n",
    "from time import sleep\n",
    "import psutil\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display image+bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_box_on_image(image, ymin, xmin, ymax, xmax,color='red',\n",
    "                               thickness=4, display_str_list=(), use_normalized_coordinates=True):\n",
    "  \"\"\"Adds a bounding box to an image.\n",
    "\n",
    "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
    "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
    "\n",
    "  Each string in display_str_list is displayed on a separate line above the\n",
    "  bounding box in black text on a rectangle filled with the input 'color'.\n",
    "  If the top of the bounding box extends to the edge of the image, the strings\n",
    "  are displayed below the bounding box.\n",
    "\n",
    "  Args:\n",
    "    image: a PIL.Image object.\n",
    "    ymin: ymin of bounding box.\n",
    "    xmin: xmin of bounding box.\n",
    "    ymax: ymax of bounding box.\n",
    "    xmax: xmax of bounding box.\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list: list of strings to display in box\n",
    "                      (each to be shown on its own line).\n",
    "    use_normalized_coordinates: If True (default), treat coordinates\n",
    "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
    "      coordinates as absolute.\n",
    "  \"\"\"\n",
    "  draw = ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  if use_normalized_coordinates:\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                  ymin * im_height, ymax * im_height)\n",
    "  else:\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "  draw.line([(left, top), (left, bottom), (right, bottom),\n",
    "             (right, top), (left, top)], width=thickness, fill=color)\n",
    "  try:\n",
    "    font = ImageFont.truetype('arial.ttf', 24)\n",
    "  except IOError:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "  # If the total height of the display strings added to the top of the bounding\n",
    "  # box exceeds the top of the image, stack the strings below the bounding box\n",
    "  # instead of above.\n",
    "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
    "  \n",
    "  # Each display_str has a top and bottom margin of 0.05x.\n",
    "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
    "\n",
    "  if top > total_display_str_height:\n",
    "    text_bottom = top\n",
    "  else:\n",
    "    text_bottom = bottom + total_display_str_height\n",
    "\n",
    "  # Reverse list and print from bottom to top.\n",
    "  for display_str in display_str_list[::-1]:\n",
    "    text_width, text_height = font.getsize(display_str)\n",
    "    margin = np.ceil(0.05 * text_height)\n",
    "    draw.rectangle([(left, text_bottom - text_height - 2 * margin), (left + text_width, text_bottom)], fill=color)\n",
    "    draw.text((left + margin, text_bottom - text_height - margin),\n",
    "              display_str, fill='black', font=font)\n",
    "    text_bottom -= text_height - 2 * margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List files in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_path = '../data/augmented'\n",
    "\n",
    "# list all files in directory\n",
    "files = sorted(os.listdir(annotated_path))\n",
    "n_files = len(files)\n",
    "print(\"Nr of files: \", n_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate annotated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 colours for the bounding boxes, based on the classes 1 to 5\n",
    "colours = {'one': 'red', 'two': 'green', 'three': 'blue', 'four': 'yellow', 'five': 'orange'}\n",
    "\n",
    "# storage path for final result\n",
    "save_path = '../data/annotated_combined/'\n",
    "\n",
    "# only use .xml files\n",
    "files_xml = [f for f in files if f.endswith('.xml')]\n",
    "\n",
    "for file_id, filename in tqdm(enumerate(files_xml), position=0, leave=True): \n",
    "    # XML object --> dict for the current file\n",
    "    # obj = xmltodict.parse(open(annotated_path + '/' + filename).read())\n",
    "\n",
    "    # read the original xml\n",
    "    xml_path = os.path.join(annotated_path, files_xml[file_id])\n",
    "    xml_file_orig = ET.parse(xml_path)\n",
    "    root = xml_file_orig.getroot()\n",
    "\n",
    "    # open image\n",
    "    image_pil = Image.open(annotated_path + '/' + filename.replace('xml', 'png'))\n",
    "\n",
    "    # # if there is only one object, wrap in list\n",
    "    # annotations = obj['annotation']['object']\n",
    "\n",
    "    # if not isinstance(annotations, list):\n",
    "    #     annotations = [annotations]\n",
    "\n",
    "    # print all objects in file\n",
    "    # loop over each bounding box\n",
    "    for obj in root.iter('object'):\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = bndbox.find('xmin')\n",
    "        xmax = bndbox.find('xmax')\n",
    "        ymin = bndbox.find('ymin')\n",
    "        ymax = bndbox.find('ymax')\n",
    "\n",
    "        xmin = int(xmin.text)\n",
    "        xmax = int(xmax.text)\n",
    "        ymin = int(ymin.text)\n",
    "        ymax = int(ymax.text)\n",
    "\n",
    "        # class name\n",
    "        class_name = obj.find('name').text\n",
    "\n",
    "        # print(\"At filename: \", filename)\n",
    "        # xmin = int(annotation['bndbox']['xmin'])\n",
    "        # ymin = int(annotation['bndbox']['ymin'])\n",
    "        # xmax = int(annotation['bndbox']['xmax'])\n",
    "        # ymax = int(annotation['bndbox']['ymax'])\n",
    "        # class_name = annotation['name']\n",
    "\n",
    "        # print(type(xmin), type(ymin), type(xmax), type(ymax))\n",
    "\n",
    "        # draw bounding boxes on image\n",
    "        draw_bounding_box_on_image(image_pil, ymin, xmin, ymax, xmax, colours[class_name],\n",
    "                                    1, display_str_list=[class_name], use_normalized_coordinates=False)\n",
    "\n",
    "        # save image\n",
    "        image_path = os.path.join(save_path, 'gt_' + str(file_id) + '_.png')\n",
    "        \n",
    "        image_pil.save(image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummaryX import summary\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "# replace the classifier for 5 fingers + background = 6 classes\n",
    "num_classes = 6 \n",
    "\n",
    "# import FCOS resnet model\n",
    "model = torchvision.models.detection.fcos_resnet50_fpn(pretrained=True, \n",
    "                                                        num_classes=91,\n",
    "                                                        pretrained_backbone=True,\n",
    "                                                        trainable_backbone_layers=4)\n",
    "classifiction_head = model.head.classification_head\n",
    "model.head.classification_head.num_classes = num_classes\n",
    "\n",
    "# conv parameters\n",
    "fout = classifiction_head.cls_logits\n",
    "\n",
    "# create new layer from parameters\n",
    "six_class_out = nn.Conv2d(in_channels=fout.in_channels, \n",
    "                        out_channels=num_classes, # now with 6 classes\n",
    "                        kernel_size=fout.kernel_size, \n",
    "                        stride=fout.stride, \n",
    "                        padding=fout.padding, \n",
    "                        dilation=fout.dilation, \n",
    "                        groups=fout.groups, \n",
    "                        padding_mode=fout.padding_mode,\n",
    "                        device=fout.weight.device,\n",
    "                        dtype=fout.weight.dtype)\n",
    "\n",
    "# replace model head with new layer\n",
    "model.head.classification_head.cls_logits = six_class_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test input\n",
    "# model.eval()\n",
    "from torch import tensor\n",
    "\n",
    "device = \"cpu\"\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "targets = [{'boxes': tensor([[115., 129., 218., 229.],\n",
    "        [430., 130., 483., 235.]], device=device), 'labels': tensor([5, 1], device=device)}, {'boxes': tensor([[137., 100., 193., 174.],\n",
    "        [407.,  92., 451., 168.]], device=device), 'labels': tensor([5, 1], device=device)}, {'boxes': tensor([[154., 144., 232., 249.]], device=device), 'labels': tensor([4], device=device)}, {'boxes': tensor([[100., 218., 199., 376.]], device=device), 'labels': tensor([2], device=device)}]\n",
    "output = model(x, targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  {'classification': tensor(1698.3805, grad_fn=<DivBackward0>), 'bbox_regression': tensor(0.4443, grad_fn=<DivBackward0>), 'bbox_ctrness': tensor(0.6356, grad_fn=<DivBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "print(\"out: \", output)\n",
    "\n",
    "# print(\"boxes shape: \", predictions[0]['boxes'].shape)\n",
    "# print(\"scores shape: \", predictions[0]['scores'].shape)\n",
    "# print(\"labels shape: \", predictions[0]['labels'].shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369441202de9f090a0e57b38848d3e186c8686d751df00b39c544f960532016f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deepl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
